
\documentclass[10pt,twocolumn]{article}

\usepackage[margin=0.75in]{geometry}
\usepackage{hyperref}
\usepackage[sorting=none]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{braket}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}

\addbibresource{references.bib}

\title{\textbf{Reproducing and Validating Fat-Tree QRAM:\\A High-Bandwidth Shared Quantum Memory Architecture}}
\date{}
\author{Alex Newsham (amn57), Oliver Fogelin (of284)}

\begin{document}

\maketitle

%=============================================================================
% SECTION 1: INTRODUCTION
%=============================================================================
\section{Introduction}

Quantum Random Access Memory (QRAM) is a critical primitive for quantum computing, enabling algorithms to query classical databases in superposition \cite{giovannetti2008quantum}. Many important algorithms---including Grover search \cite{grover1996search}, HHL for solving linear systems \cite{harrow2009quantum}, Hamiltonian simulation \cite{low2019hamiltonian}, and quantum machine learning schemes \cite{biamonte2017quantum}---assume QRAM as an ideal primitive and measure complexity only in terms of query count.

The prevailing architecture for QRAM is bucket-brigade (BB) QRAM \cite{giovannetti2008quantum,giovannetti2008architectures}, which uses a binary tree of quantum routers to achieve $O(\log N)$ query latency for a database of size $N$ using $O(N)$ qubits. However, BB QRAM has a significant limitation in shared-memory settings: a single query commits all router hardware for its duration, with the root node serving as the sole external interface through which all address and bus qubits must pass, forcing concurrent queries to serialize. For $p$ clients, effective latency grows to $O(p \log N)$, making QRAM a bandwidth bottleneck for parallel quantum workloads.

Fat-Tree QRAM, proposed by Xu et al.\ \cite{xu2025fat}, addresses this limitation by enabling query-level parallelism while retaining BB's space and error scaling. The architecture duplicates routers along an additional index and interleaves controlled-SWAP operations with SWAP layers according to a scheduling protocol. This allows up to $O(\log N)$ concurrent queries to coexist in different slices of the structure, achieving $O(\log N)$ total latency for $O(\log N)$ queries---giving bandwidth that is asymptotically independent of $N$.

In this work, we reproduce and validate the core claims of Fat-Tree QRAM at the circuit level. Our contributions are:
\begin{enumerate}
    \item A Qiskit-based implementation of both BB and Fat-Tree QRAM architectures.
    \item An implementation of Algorithm 1 from Xu et al., the query scheduling protocol that enables pipelined execution.
    \item Validation of correctness via quantum simulation, comparing pipelined multi-query behaviour against sequential execution.
    \item Verification of resource scaling claims, measuring qubit counts, circuit depth, and gate counts as functions of $N$.
\end{enumerate}

\textcolor{red}{\textbf{ALEX: INSERT BRIEF SUMMARY OF RESULTS HERE PLS MAYBE}}

%=============================================================================
% SECTION 2: BACKGROUND (~1 page)
%=============================================================================
\section{Background}

%-----------------------------------------------------------------------------
\subsection{Quantum Random Access Memory}

A QRAM provides quantum algorithms with the ability to query a classical database of $N$ entries in superposition. Formally, the QRAM query operation is defined as:
\[
\sum_{i=0}^{N-1} \alpha_i \ket{i}_A \ket{0}_B \;\longmapsto\; \sum_{i=0}^{N-1} \alpha_i \ket{i}_A \ket{x_i}_B
\]
where $\ket{i}_A$ is an $n = \log_2 N$ qubit address register prepared in a superposition with amplitudes $\alpha_i$, $\ket{x_i}_B$ is a bus register that receives the data, and $\{x_i\}_{i=0}^{N-1}$ is the classical database. In this work, we consider a single-qubit bus (i.e., each $x_i \in \{0,1\}$).

The key property of QRAM is that it maintains coherence across the superposition: a query to an address in superposition returns the corresponding data values entangled with each address component. Classical RAM cannot operate on addresses in superposition, returning only a single value per query rather than data entangled with each address component. QRAM thus enables quantum algorithms to access data in ways that exploit quantum parallelism.

%-----------------------------------------------------------------------------
\subsection{Bucket-Brigade QRAM}

Bucket-brigade (BB) QRAM \cite{giovannetti2008quantum,giovannetti2008architectures} arranges $2^n - 1$ quantum routers in a binary tree structure above the classical memory cells. Figure~\ref{fig:bb-qram}(c) shows this H-tree layout for $N=8$: routers (circles) sit above the data cells $x_0, \ldots, x_7$, with the address and bus qubits entering at the root.

\subsubsection{Router Structure and Operations}
\label{sec:router-ops}

Each router is modeled as a unit with four logical ports: an \emph{input} port that receives incoming qubits, a \emph{route} register that stores the routing decision, and \emph{left}/\emph{right} output ports connected to child routers. The router operates in one of three states: $\ket{0}$ (route left), $\ket{1}$ (route right), or $\ket{W}$ (wait). The wait state $\ket{W}$ indicates an inactive router that has not yet received an address bit. In our implementation, we encode $\ket{W}$ simply as $\ket{0}$: since all qubits initialize to $\ket{0}$ and the route operation uses controlled-SWAPs, an inactive router with route${}=\ket{0}$ will route any incoming qubit leftward by default. This is correct because qubits only reach a router after address bits have been loaded into all ancestor routers, ensuring proper activation order.

Four primitive operations define router behaviour:
\begin{itemize}
    \item \textbf{Load}: Transfer a qubit from an external source (address bus or parent router) into the router's input port. Implemented as a SWAP between the source and input.
    \item \textbf{Store}: Transfer the qubit from the router's input port into its route register, storing the routing decision. Implemented as a SWAP between input and route.
    \item \textbf{Route}: Direct an incoming qubit toward the appropriate child based on the stored route bit. If route $= \ket{0}$, CSWAP the input toward the left child; if route $= \ket{1}$, toward the right child. This is implemented as two CSWAPs with an X gate: \texttt{CSWAP(route, input, right); X(route); CSWAP(route, input, left); X(route)}.
    \item \textbf{Transport}: Move qubits between adjacent routers via SWAP operations, enabling bit-level pipelining.
\end{itemize}

\subsubsection{Query Execution}

A query proceeds in three phases, illustrated in Figure~\ref{fig:bb-qram}(a) for $N=8$. During \textbf{address loading}, each address bit $a_i$ is loaded into the root and routed down through previously-activated routers to level $i$, where it is stored. Each router at level $i$ stores address bit $a_i$ and uses it to route subsequent qubits left (if $a_i = 0$) or right (if $a_i = 1$), as shown in Figure~\ref{fig:bb-qram}(b). During \textbf{data retrieval}, a bus qubit follows the activated paths to the leaves, where data-controlled X gates flip it according to the classical data values, then returns up the tree. During \textbf{address unloading}, address bits are extracted back out through the root, restoring all routers to $\ket{W}$. For an $n$-bit address, each phase requires $O(n)$ layers, giving $O(\log N)$ total circuit depth.

When the address register is in superposition over multiple addresses, each router at level $i$ enters a superposition of $\ket{0}$ and $\ket{1}$ corresponding to the $i$-th bit of each address component. Crucially, the routing is \emph{coherent}: each branch of the superposition activates only \emph{one} root-to-leaf path, not all possible paths simultaneously. For example, if the address is $\frac{1}{\sqrt{2}}(\ket{010} + \ket{111})$, only two paths are activated (addresses 2 and 7), while all other routers remain in $\ket{W}$.

\subsubsection{Error Scaling}

The coherent routing property is essential for BB QRAM's favourable error scaling \cite{arunachalam2015robustness}. Although the tree contains $O(2^n)$ routers, each amplitude component of a superposition query activates exactly $n$ routers (one per level along a single root-to-leaf path). If each router operation introduces error $\varepsilon$, the total query infidelity scales as $O(\log^2 N \cdot \varepsilon)$, not $O(N \cdot \varepsilon)$. This poly-logarithmic error scaling makes BB QRAM practical for large memories, provided individual router fidelities are sufficiently high.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{bb_qram_figure.jpg}
    \caption{Bucket-Brigade QRAM architecture. (a) Query execution for $N=8$ showing circuit layers. (b) A single router using CSWAP operations. (c) H-tree layout with active routers (red) after address loading. Figure reproduced from Xu et al.\ \cite{xu2025fat}.}
    \label{fig:bb-qram}
\end{figure}

%-----------------------------------------------------------------------------
\subsection{Fat-Tree QRAM}

The key limitation of BB QRAM in shared-memory settings is that a single query commits all router hardware for its duration, with the root node acting as the sole interface through which every qubit must pass, blocking concurrent access. Fat-Tree QRAM \cite{xu2025fat} overcomes this by \emph{duplicating} routers along a new index $k$, creating a structure where multiple queries can coexist in different ``slices'' of the architecture without conflict.

\subsubsection{Router Duplication and Structure}

Fat-Tree QRAM retains the same binary tree layout as BB QRAM, with $2^i$ nodes at level $i$. However, each node at level $i$ contains $(n-i)$ quantum routers instead of one. Routers are indexed by a 3-tuple $(i, j, k)$: $i \in [0, n-1]$ is the level, $j \in [0, 2^i - 1]$ is the node index, and $k \in [0, n-i-1]$ identifies the router copy within node $(i, j)$. Thus:
\begin{itemize}
    \item Level 0 (root): 1 node with $n$ routers
    \item Level 1: 2 nodes, each with $n-1$ routers
    \item Level $n-1$ (leaves): $2^{n-1}$ nodes, each with 1 router
\end{itemize}

Figure~\ref{fig:fat-tree} illustrates this structure for $N=32$. Each internal node contains multiple quantum routers (shown as coloured circles), with the number of routers per node decreasing linearly toward the leaves. The total router count is $\sum_{i=0}^{n-1} (n-i) \cdot 2^i = 2N - 2 - n$, approximately twice BB QRAM's $2^n - 1$ routers.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fat_tree_figure.jpg}
    \caption{Layout of Fat-Tree QRAM with capacity $N=32$. Classical data are located at the leaves and internal nodes contain multiplexed quantum routers. Colours indicate connections between routers across the tree. The number of routers per node increases linearly toward the root. Figure reproduced from Xu et al.\ \cite{xu2025fat}.}
    \label{fig:fat-tree}
\end{figure}

\subsubsection{Query Pipelining via SWAP Layers}

The key insight enabling parallelism is that queries at different stages of execution can occupy different $k$-slices simultaneously. Fat-Tree QRAM achieves this through an alternating schedule of \textbf{gate steps} and \textbf{SWAP layers}:

\begin{itemize}
    \item \textbf{Gate steps}: Perform standard BB QRAM operations (load, route, store) within a fixed $k$-slice. Each query operates on routers at its current $k$-level.
    \item \textbf{SWAP-I layers}: Exchange router contents between adjacent $k$-levels where $k$ is even (i.e., between $k=0 \leftrightarrow k=1$, $k=2 \leftrightarrow k=3$, etc.).
    \item \textbf{SWAP-II layers}: Exchange router contents between adjacent $k$-levels where $k$ is odd (i.e., between $k=1 \leftrightarrow k=2$, $k=3 \leftrightarrow k=4$, etc.).
\end{itemize}

The execution alternates: gate step $\rightarrow$ SWAP-I $\rightarrow$ gate step $\rightarrow$ SWAP-II $\rightarrow$ repeat. This pattern ensures that queries ``move'' through $k$-space as they progress, vacating lower $k$-levels for new incoming queries. A new query can be injected once per four-timestep scheduling cycle, allowing up to $O(n) = O(\log N)$ queries to be in flight simultaneously.

\subsubsection{Parallelism and Bandwidth}

Because queries occupy disjoint $k$-slices at any given time, they do not conflict on router resources. The result is that $O(\log N)$ independent queries complete in $O(\log N)$ total time steps, giving an effective per-query latency of $O(1)$ in the amortized sense. This represents a bandwidth improvement of $O(\log N)$ over BB QRAM, where $p$ queries require $O(p \log N)$ time.

The trade-off is space: Fat-Tree QRAM requires approximately twice as many routers as BB QRAM. However, for shared-memory workloads with high query contention, this modest space overhead is justified by the dramatic improvement in throughput.

%=============================================================================
% SECTION 3: IMPLEMENTATION (~1 page)
%=============================================================================
\section{Implementation}

We implemented both BB QRAM and Fat-Tree QRAM in Python using the Qiskit quantum circuit library. The complete source code is available at \url{https://github.com/olifog/fat-tree-qram}. This section describes our code architecture and the key implementation decisions.

%-----------------------------------------------------------------------------
\subsection{Code Architecture}

Our implementation follows a modular design separating concerns across several components. The \texttt{core} module provides foundational abstractions: a \texttt{Router} dataclass representing individual quantum routers with their level, node index, and qubit references; a \texttt{RouterTree} class that manages the binary tree structure and provides traversal methods; and an \texttt{operations} module containing the four primitive router operations (load, store, route, transport) as simple Qiskit gate sequences.

The \texttt{BucketBrigadeQRAM} class implements standard BB QRAM using the \texttt{RouterTree} abstraction, with methods for address loading, data retrieval, and address unloading that closely follow the three-phase query procedure described in Section~\ref{sec:router-ops}. The \texttt{FatTreeQRAM} class implements the Fat-Tree architecture with its additional router copies indexed by $k$, along with \texttt{swap\_i} and \texttt{swap\_ii} methods for the interleaved SWAP layers. A separate \texttt{FatTreeScheduler} class implements Algorithm 1 from Xu et al., managing the pipelining state machine that tracks each query's progress through the load/unload phases and coordinates SWAP layer timing.

The test suite validates correctness through simulation using Qiskit's \texttt{AerSimulator}, testing both classical address queries and superposition queries for various data patterns. Comparison tests verify that BB and Fat-Tree implementations produce identical results for single queries.

%-----------------------------------------------------------------------------
\subsection{Classical Data Application}

The classical database values $\{x_i\}$ must be applied to the bus qubit when it reaches the leaf level. Each leaf router at node index $j$ has its left port corresponding to data index $2j$ and its right port to data index $2j+1$. The bus qubit, routed to either the left or right port based on the final address bit, picks up the data value when X gates conditionally flip the port contents.

In our BB QRAM implementation, we apply X gates to the leaf router ports twice---once after the bus reaches the leaves and once after the return routing operation at the leaf level. For data value 1, the first X gate sets the port to $\ket{1}$; the subsequent CSWAP transfers this to the bus; the second X gate then flips the port again. The net effect is that the bus acquires the data value via XOR, while the leaf ports are left in a state corresponding to their data values rather than being restored to $\ket{0}$.

For Fat-Tree QRAM, we use a cleaner approach: at each SWAP layer when queries occupy the $k = n-1$ slice (which contains the leaf-level routers), we reset the left/right ports to $\ket{0}$ and conditionally apply X gates based on the classical data values. As specified in Algorithm 1 of Xu et al., this occurs at SWAP-I layers when $n$ is odd and at SWAP-II layers when $n$ is even.

%-----------------------------------------------------------------------------
\subsection{Indexing the Fat-Tree Structure}

\textcolor{red}{\textbf{ALEX: could u pls explain the $(k, \text{tree}, \text{level}, \text{node})$ coordinates into 1D array translation pls}}

%=============================================================================
% SECTION 4: RESULTS
%=============================================================================
\section{Results}

%-----------------------------------------------------------------------------
\subsection{Correctness Validation}
%-----------------------------------------------------------------------------
\subsection{Resource Scaling}
%-----------------------------------------------------------------------------
\subsection{Parallel Query Performance}
%-----------------------------------------------------------------------------

%=============================================================================
% SECTION 5: CONCLUSION
%=============================================================================
\section{Conclusion}

our implementation is awesome

% - How well do empirical results match paper's claims?
% - Where do discrepancies arise (gate counting conventions, etc.)?
% - Practical implications
% - How this applies to like quantum architecture in general

%=============================================================================
% REFERENCES
%=============================================================================
\printbibliography

\end{document}
